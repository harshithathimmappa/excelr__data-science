# -*- coding: utf-8 -*-
"""DECISION TREE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hlfouu5y1vNXE2i6_RY-DtR4M0kukLJi
"""

##QUESTION 1 COMPANY

import pandas as pd
import matplotlib.pyplot as plt  
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import  DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import classification_report
from sklearn import preprocessing

data=pd.read_csv("Company_Data.csv",sep=',')
data.head(10)

data=pd.read_csv("Company_Data.csv",sep=',')
data.head(10)

data.shape

data.describe()

import seaborn as sns
sns.set_style(style='dark')
sns.pairplot(data)

data.ShelveLoc.unique()

data.Sales.mean()

## if sales greater than 7.5 then it is high sales
## if sales lesser than 7.5 then it is low sales

label_encoder = preprocessing.LabelEncoder() 
data['Sales'] = data.Sales.map(lambda x: 1 if x >=7.5 else 0)
data['ShelveLoc']= label_encoder.fit_transform(data['ShelveLoc']) 
data['US']=label_encoder.fit_transform(data['US'])
data['Urban']=label_encoder.fit_transform(data['Urban'])

data.head(10)

x=data.iloc[:,1:11]
y=data['Sales']

x 
pd.set_option("display.max_rows", None)

x

y

data['Sales'].unique()

data.Sales.value_counts()

colnames = list(data.columns)
colnames

x_train, x_test,y_train,y_test = train_test_split(x,y, test_size=0.2,random_state=50)

##Building Decision Tree Classifier using Entropy Criteria

model = DecisionTreeClassifier(criterion = 'entropy',max_depth=3)
model.fit(x_train,y_train)

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(model);

model.feature_importances_

fn=['CompPrice','Income','Advertising','Population','Price','ShelveLoc','Age','Education','Urban','US']

import pandas as pd
feature_imp = pd.Series(model.feature_importances_,index=fn).sort_values(ascending=False) 
feature_imp

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

preds = model.predict(x_test) 
pd.Series(preds).value_counts()

preds

pd.crosstab(y_test,preds)

np.mean(preds==y_test)

##Bagging decision tree

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

label_encoder = preprocessing.LabelEncoder() 
data['Sales'] = data.Sales.map(lambda x: 1 if x >8 else 0)
data['ShelveLoc']= label_encoder.fit_transform(data['ShelveLoc']) 
data['US']=label_encoder.fit_transform(data['US'])
data['Urban']=label_encoder.fit_transform(data['Urban'])

data.head(10)

a=data.iloc[:,1:11]
b=data['Sales']
kfold = KFold(n_splits=10)
cart = DecisionTreeClassifier()
num_trees = 100
model_1 = BaggingClassifier(base_estimator=cart, n_estimators=num_trees,random_state=0)
results = cross_val_score(model_1, a, b, cv=kfold)
print(results.mean())

model_1= model_1.fit(x_train,y_train) #train decision tree
y_predict = model_1.predict(x_test)

y_predict

np.mean(y_predict==y_test)

##Building Decision Tree Classifier (CART) using Gini Criteria

from sklearn.tree import DecisionTreeClassifier
model_gini = DecisionTreeClassifier(criterion='gini', max_depth=3)

model_gini.fit(x_train, y_train)

pred=model.predict(x_test)
np.mean(preds==y_test)

model_gini.feature_importances_

cn=['CompPrice','Income','Advertising','Population','Price','ShelveLoc','Age','Education','Urban','US']

feature_imp = pd.Series(model_gini.feature_importances_,index=cn).sort_values(ascending=False) 
feature_imp

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

preds_1 = model_gini.predict(x_test) 
pd.Series(preds_1).value_counts()

np.mean(preds_1==y_test)

##QUESTION 2 FRAUD

import pandas as pd
import matplotlib.pyplot as plt  
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import  DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import classification_report
from sklearn import preprocessing

fraud=pd.read_csv("Fraud_check.csv")
fraud.head(10)

fraud.dtypes

fraud.shape

fraud.describe()

plt.boxplot(fraud['Taxable.Income'])
plt.show()

fraud["tax_income"]="<=30000"

fraud.loc[fraud["Taxable.Income"]>=30000,"tax_income"]="Good"
fraud.loc[fraud["Taxable.Income"]<=30000,"tax_income"]="Risky"

fraud["tax_income"].unique()

fraud.head(10)

fraud=fraud.drop(["Taxable.Income"],axis=1)

fraud.head(5)

fraud.rename(columns={"Undergrad":"UG","Marital.Status":"Marital","City.Population":"Population","Work.Experience":"exp"},inplace=True)

fraud

label_encoder = preprocessing.LabelEncoder() 
fraud['UG']= label_encoder.fit_transform(fraud['UG']) 
fraud['tax_income']=label_encoder.fit_transform(fraud['tax_income'])
fraud['Urban']=label_encoder.fit_transform(fraud['Urban'])
fraud['Marital']=label_encoder.fit_transform(fraud['Marital'])

fraud.head(10)

fraud.dtypes

x=fraud.iloc[:,0:5]
y=fraud['tax_income']

x

y

fraud['tax_income'].unique()

fraud.tax_income.value_counts()

colnames = list(fraud.columns)
colnames

x_train, x_test,y_train,y_test = train_test_split(x,y, test_size=0.3,random_state=25)

model = DecisionTreeClassifier(criterion = 'entropy',max_depth=3)
model.fit(x_train,y_train)

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)
tree.plot_tree(model);

model.feature_importances_

fn=['UG','Marital','Population','exp','Urban']

feature_imp = pd.Series(model.feature_importances_,index=fn).sort_values(ascending=False) 
feature_imp

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

preds = model.predict(x_test) 
pd.Series(preds).value_counts()

preds

pd.crosstab(y_test,preds)

print(y_test)

np.mean(preds==y_test)

##Building Decision Tree Classifier (CART) using Gini Criteria

from sklearn.tree import DecisionTreeClassifier
model_gini = DecisionTreeClassifier(criterion='gini', max_depth=3)

model_gini.fit(x_train, y_train)

pred_1=model_gini.predict(x_test)
np.mean(preds==y_test)

model_gini.feature_importances_

